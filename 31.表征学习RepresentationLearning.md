## RNN
RNN encodes state (memory) from character to character in weights
RNN retains state (memory) by updating its weights dynamically

the behaviour of the recurrent network can be described as a dynamical system by the pair of non-linear matrix equations
$$s_t=f(Ux_t+Ws_{t-1})\\
o_t=g(Vs_t)$$
the state is defined by the set of hidden unit activations $s$. RNN retains state (memory) by updating its weights dynamically
Note that the weight matrices $U,W,V$ are shared across layers 
![](\images\rnn_unfold.jpg)
![](\images\rnn_example.png)

## LSTM
LSTMs are a more general form of RNN that explicitly encode a memory state ‘C’ (rather than rely on weight updates)
![](/images/lstm.png)
Gates control information flow
The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. 
The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through.
![](/images/lstm_gate.png)
The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” 
![](/images/lstm_1.png)
The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, $C_t$, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.
![](/images/lstm_2.png)
It’s now time to update the old cell state, $C_{t−1}$, into the new cell state $C_t$. We multiply the old state by $f_t$, forgetting the things we decided to forget earlier. Then we add $i_t∗C_t$. This is the new candidate values, scaled by how much we decided to update each state value.
![](/images/lstm_3.png)
Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.
![](/images/lstm_4.png)

http://colah.github.io/posts/2015-08-Understanding-LSTMs/
https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html

## Visual Question Answering (VQA)
combine visual (CNN) and text sequence (LSTM) networks to generate text from images (automatic scene description)
![](/images/vqa.png)
Freeform VQA
mapping an image and question to a feature embedding (‘fact embedding’)
![](/images/vqa1.png)
Limitations
* Answers are very repetitive (c.f. texture synthesis in DC-GAN)
* Not much variation in answer style (LSTM answer channel).
* LSTM in general can only learn temporally local structure
* No creativity – must have experienced visual cue/answer pair, no scope for generalisation beyond taught concepts.
* Scene description can be inverted to scene synthesis from text e.g. conditional GANs

## Unsupervised Representation Learning
Encoder-decoder FCN can be used as an autoencoder (AE) to learn a suitable bottleneck (latent) representation for an unannotated dataset.
![](/images/ae1.png)
## Supervised Representation Learning
We can only control the nature of representation by manipulating the architecture.  If we can, better to supervised the learning with +/- examples
![](/images/ae2.png)
## SimCLR - Contrastive Learning
SimCLR is a recent (2020) paper listing tricks for unsupervised learning.
* Data augmentation
    * Crop patches from images in the batch and colour jitter them
* With-in batch sampling of positive/negative
    * 2N such patches from N images in the batch. 
    * Consider patches from the same image as a positive pair (a single pair)
    * All the other patches in the batch are negatives
* MLP layer to compute loss instead of embedding itself
    * Use a ‘head’ network g(.) basically 1-2 MLP layer (giving 10% and 3% boost)
## MoCo – Momentum Contrastive Learning
Similar to SimCLR.  An anchor, a positive and rich set of negatives. But negatives are sampled from previous epochs in a queue
![](\images\moco.png)
![](\images\moco1.png)
## RNN + Autoencoder
![](\images\rnn_ae.png)
## RNN+ VAE
A Variational Autoencoder is a kind of non-deterministic AE in which we learn a mean and covariance to drive decoder stage

we simply cannot do back-propagation for a random sampling process. Fortunately, we can leverage a clever idea known as the "reparameterization trick" which suggests that we randomly sample e from a unit Gaussian, and then shift the randomly sampled m by the latent distribution's mean m and scale it by the latent distribution's variance σ.
![](\images\vae.png)
变分自编码器便是用“取值的概率分布”代替原先的单值来描述对特征的观察的模型，如下图的右边部分所示，经过变分自编码器的编码，每张图片的微笑特征不再是自编码器中的单值而是一个概率分布。实质上实施了连续，平滑的潜在空间表示
![](\images\vae1.png)
A single input could generate several similar outputs capturing variation in the dataset (e.g. for sketch)
![](\images\rnn_vae.png)
https://www.jeremyjordan.me/variational-autoencoders/
https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf


## Deep Image Prior
Deep Image Prior is a type of convolutional neural network used to enhance a given image with no prior training data other than the image itself. A neural network is randomly initialized and used as prior to solve inverse problems such as noise reduction, super-resolution, and inpainting.

The fact that we recognize one as a natural image and the other as noise indicates that humans have some implicit prior over what natural images should look like.

An untrained deep convolutional generator can be used to replace the surrogate natural prior (the TV norm) with dramatically improved results.
$x$ -> clean image
$\dot x$ → degraded image
$x^*$ → restored image

$p(x|\dot x)=\alpha p(\dot x|x)p(x)$

$x^*=\argmax p(x|\dot x)\\= \argmin \big(-log(p(\dot x|x))-log(p(x))\big)\\=\argmin \big( E(x,\dot x) + R(x)\big)$

Encoder-decoder overfitted to an image in order to learn weights $\theta$ necessary to reconstruct it from white noise.
![](\images\dip.png)
例如，给定一幅被破坏的图像x，具体过程如下：
1. 用随机参数初始化深度卷积网络f。
2. 令f的输入为固定的随机编码z。
3. 令f的目标为：输入z，输出x。以此训练f的参数。
4. 注意选择合适的损失函数。例如对于降噪问题可关注整体的MSE，对于填充问题就应该只关心不需要填充的位置的MSE。
5. 当训练很久之后，f可实现输出一模一样的x。但如果在训练到一半时打断f，会发现它会输出一幅“修复过的x”。

这意味着，深度卷积网络先天就拥有一种能力：它会先学会x中“未被破坏的，符合自然规律的部分”，然后才会学会x中“被破坏的部分”。例如，它会先学会如何复制出一张没有噪点的x，然后才会学会复制出一张有噪点的x。

相对于高频的、无序（信息熵高）的噪声，低频、有规则的自然图像是更容易被编码的。通过训练，CNN能很自然地提取出自然图像中普遍的特征，完成生成干净图像的任务。而对于剩余的噪声，则是要依靠网络的死记硬背来完成（在很多任务里这种情况就属于过拟合了），于是就需要更久的训练才能实现。

https://www.zhihu.com/question/263404981/answer/269351837