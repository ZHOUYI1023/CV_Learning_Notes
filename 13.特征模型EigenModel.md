## Eigenvalue and Eigenvector
$Av = \lambda v$
$det(A-\lambda I) =0 $

$A\cdot V = V \cdot D$
$A = VDV^T$
where D is a diagonal matrix of eigenvalues and V is a matrix whose columns are the corresponding eigenvectors

Different eigenvectors (columns of V) are orthonormal to each other, which can be considered as a set of basis vector

## Eigenmodel
* Step 1: Arrange n (for example, 2 dimensional) data vector into matrix
$x = \begin{bmatrix}
x_1 & x_2 & x_3 & ... & x_n \\
y_1 & y_2 & y_3 & ... & y_n
\end{bmatrix}
$
* Step 2: Calculate the mean in each dimension
* Step 3: Calculate the covariance matrix
$C = \frac{1}{n}(x-\mu)(x-\mu)^T$
* Step 4: Decompose the covariance matrix using EVD
```matlab
[V,D] = eig(C)
```
The Eigenmodel allows us to extract features from data 
For classification problem, we need to define a distance
## Mahalanobis Distance
Intuitions:
Non-uniform distribution of training data, thus we need an ellipsoid decision boundary.

To measure the ditribution, we use variance (1-D) / covaraince (N-D)
* The eigenvectors form a data-dependent basis set (orthonormal)
* The eigenvalues tell us how much variance there is along each corresponding eigenvector.

Thus measure distance in units of standard deviation using Mahalanobis distance 

Process:
1. Substract mean, get a vector in centroid spheroid frame $x := x- \mu$
2. project vector into in centroid ellipsoid frame by $V(x-\mu)$
one point in two frames problem
suppose the new coordinate at eigenvector frame is denoted by $x'$
$Vx' = x, x' = V^{-1}x = V^T x$

3. penalize the standard deviation, by $D^{-1}x'$, then the vector is in a centroid spheroid again
4. to get a distance function we use 
$(D^{-1}V^T(x-\mu))^T D^{-1}V^T(x-\mu)$
$= (x-\mu)^TVD^{-T}D^{-1}V^T(x-\mu)$
5. $D^{-T}D^{-1} = D^{-1}D^{-1} = U^{-1}$ is the matrix of eigenvalue
Covariance matrix is given by 
$C = VUV^T, C^{-1} = VU^{-1}V^T$

6. the Mahalanobis distance is given by
$d^2 = (x-\mu)^TC^{-1}(x-\mu)$



## Principal Component Analysis
* Inutuition: 
    * deal with Curse of Dimensionality
    * some features are correlated

* Process:
    * Step 1: Extract the eigenvalues and eigenvectors of the covariance matrix
    * Step 2: Select top percentage of eigenvalues, up to say 97% of trace
    * Step 3: Drop the null space of eigenvector matrix to get K 
    Notice that K is not square but it columns represent orthonormal basis, thus we can get
    $q =  K^{-1} p = K^T p$
![](\images\pca.png)

![](\images\visual.png)