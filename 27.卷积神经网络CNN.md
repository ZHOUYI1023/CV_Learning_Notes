## Layer Architecture
### MLP
Neuron
![](\images\neuron.png)
The output of an MLP neuron is a weighted sum of its inputs, normalised via a sigmoid function.  
$h_{\theta}(x) = \frac{1}{1+e^{-\theta^T x}}$
### Convolution Neural Network
AlexNet consists of 5 Convolutional Layers and 3 Fully Connected Layers
![](\images\alexnet3.png)
![](\images\alexnet1.png)
#### Convolution layer
![](\images\convnet.png)
#### ReLU (rectified linear units) layer
ReLu is fundamental to a CNN’s ability to learn complex functions – breaks associativity of successive convolutions
$R(z) = max(0,z)$
![](\images\relu.png)
#### Max pooling layer
![](\images\pooling.png)
#### Fully connected layer 
“FC layer” = “inner product layer” = “dense layer”

#### Softmax layer
sigmoid binary classification
Softmax is kind of Multi Class Sigmoid, but if you see the function of Softmax, the sum of all softmax units are supposed to be 1.
$S(z_j) = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}$

### Batch Normalization
Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. 

* deal with internal covariate shift
* use before(more popular) or after the activation function
* cannot be applied on small batch
* do not use batch normalization and dropout in the same network

Training:
![](\images\batch.png)
Testing:
![](\images\batch1.png)

* less covariate shift, larger learning rate
* less exploding/vanishing gradients especially for sigmoid, tanh.etc
* less affected by weight initialization
* reduce the demand for regularization

### Computational Complexity
![](\images\complexity.jpg)
multiply-add counts(MAC)算一次操作，卷积层 computational complexity 等于上图中两个立方体 (绿色和橙色) 体积的乘积。
https://www.zhihu.com/question/65305385?utm_source=wechat_session&utm_medium=social&utm_oi=1045718286184124416

## LeNet (1998)
AlexNet(2012)
![](\images\alexnet.png)
CNN as a Descriptor + SVM classifier
## VGG(2015)
## GoogLeNet (2015)
![](\images\inception.png)
![](\images\inception1.png)
![](\images\googlenet.png)
## Forward
![](\images\cnn_conv.png)



## Training
Each iteration of training using all data is an “epoch”
A “batch” (sometimes “minibatch”) of data (e.g. 100 examples) is chosen at random from all training data – it usually takes several batches to train an epoch

The batch size is a number of samples processed before the model is updated.
* Batch Gradient Descent. Batch Size = Size of Training Set
* Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training Set. A good default for batch size might be 32.
* Stochastic Gradient Descent. Batch Size = 1. However, SGD almost always actually refers to mini-batch gradient descent.

Feature scaling
![](\images\featurescaling.png)
```(image - image.mean()) / (image.std() + 1e-8)```

PCA/Whiten
![](\images\whiten.png)

The training set of CIFAR-10 is of size 50,000 x 3072, where every image is stretched out into a 3072-dimensional row vector. We can then compute the [3072 x 3072] covariance matrix and compute its SVD decomposition (which can be relatively expensive). 
![](\images\whiten1.png)


### Backpropagation
Softmax Logloss: wrong answer $p_k$, right answer $-(1-p_k)$ 
ReLu: Just pass through the gradient from above layer or a zero
Max-Pooling: gradient only propagates back to the max (need to record the locations) 
FC: 无论是MLP还是卷积计算,所涉及的数学计算都只是多项式的加乘，所以当你去计算一个值输出的导数时，最好的办法就是去找到有该值贡献的输出值和对应的权值，你也就找到了相应的导数。
Conv: we need to collect loss from each of the output images and aggregate into the weight vector

### Weight Initialization
Small network: Gaussian noise zero mean and 0.01 variance
Deep network: dynamic $\sigma$
* Xavier $\sigma = \frac{1}{n_{in}+n_{out}}$ where n is the number of image channels 

* $\sigma = \sqrt{\frac{2}{FilterSize*n_{out}}}$

### Fine-tuning
Obtain a pre-trained network over your domain (e.g. ImageNet)
Freeze weights in first 3-5 conv layers (experimenting needed)
* Use white noise to init remaining layers
* Or (more commonly) continue back-prop from ImageNet derived weights

### Learning Curve
* the validation loss should converge (but is usually higher than) the training loss
* loss can help us adjust learning rate, or indicate when to stop training
* continued training of a good fit will likely lead to an overfit.
![](\images\underoverfitting.png)
(do not confused with the learning curve(high bias high/variance) whose x axis is datasize)

* Optimization Learning Curves: Learning curves calculated on the metric by which the parameters of the model are being optimized, e.g. loss.
* Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.