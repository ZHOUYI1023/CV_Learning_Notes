# Lecture 19 Segmentation I
# Top-down vs bottom-up segmentation
* Top-down: pixels belong together because they are from the same object
* Bottom-up: pixels belong together because they look similar

Bottom-up segmentation via clustering– Algorithms: 
* Mode finding and mean shift: k-means, EM, mean-shift
* Graph-based: normalized cuts 
Segmentation Cues:
* color
* texture
* motion
* depth
## Binary segmentation
### global thresholding
color eigenmodel + mahalanobis distance
* object (white/1)    
* non-object / background (black/0)
### dynamic thresholding
sliding window 
threshold relative to pixel value within the window
works on any data where low-freq variation is undesired
do some morphological operations 

## Multi-label: Split and Merge
* SPLIT
Repeatedly split image into regions until each region is sufficiently homogeneous (e.g. variance below some threshold)：==QuadTree==

* MERGE
For each leaf of the tree, check if it can be merged with other leaves and retain variance below threshold. It mitigates over-splitting

## Segmentation via K-Means
* RGB
lacks any spatial coherence
over-segmentation and rough region boundaries
* RGBXY
pixels with similar colours AND similar locations would group together  

## Mean Shift
Finding modes in a set of data samples, manifesting an underlying probability density function (PDF)
The mean shift algorithm seeks modes or local maxima of density in the feature space
![](images\mean_shift.png)
1. Pick k points (means) to act as cluster centres, just like k-means
2. For each point, compute density in a small local window
3. Shift the point (mean) in direction of gradient(center of mass) until convergence, i.e. the center of circle coincide with the center of mass
4. If windows of two points overlap enough, merge those means into one mean.

Mean shift clustering/segmentation
1. Find features (color, gradients, texture, etc)
2. Initialize windows at individual feature points
3. Perform mean shift for each window until convergence
4. Merge windows that end up near the same “peak” or mode

The window (kernel) used in Mean Shift could have any form so long as the area under it sums to 1, e.g. rectangle and Gaussian

Parzan function:
A function of some finite number of data points x1…xn 
(xi is a multi-dimensional constant data point)
$f(x) = \frac{1}{n}\displaystyle\sum_{i=1}^n K(x-x_i)$
using the kernal as
$K(x-x_i) = ck(||\frac{x-x_i}{h}||^2)$
where h is the bandwidth i.e. range of the kernel; xi is the mean
$\nabla f(x) =  \frac{1}{n}\displaystyle\sum_{i=1}^n \nabla K(x-x_i) \newline = \frac{c}{n}(\sum g_i)(\frac{\sum x_ig_i}{\sum g_i}-x)$

where $g = -k'(x)$

$\vec m_h(x)=\frac{\sum_{i=1}^ng(\frac{||x-x_i||^2}{h}) x_i}{\sum_{i=1}^ng(\frac{||x-x_i||^2}{h})}-x$

![](images\mean_shift_procon.png)
![](images\mean_shift_eg.png)
Flat regions induce the modes
Feature space : Joint domain = spatial coordinates(x-y plane) + grey-level color space(z-axis)
![](images\mean_shift_eg1.png)

## Berkeley Methodology

Precision:  The fraction of boundary pixels in the machine image that are “correct” i.e. also boundaries in the ground truth

Recall:  The fraction of ground truth boundaries that are returned as boundaries in the machine image.

Compute P-R for several experimental conditions of your algorithm 
* $Precision = \frac{TP}{TP+FP}$ 
* $Recall = \frac{TP}{TP+FN}$ 
![](images\PRSeg.png)

F-measure (F1 score): harmonic mean of precision and recall
$F_1 = 2\frac{P\cdot R}{P+R}$


---

# Lecture 20 Segmentation
Binary segmentation: 
* foreground: source
* background: sink
## Min Cut
Images as graphs：
* node (vertex) for every pixel
* link between every pair of pixels
* affinity weight for each link (edge)

The cost of a cut is the sum of weights (similarities) removed by the cut

$cut(A,B) = \displaystyle\sum_{p\in A, q\in B}c_{p,q}$
where A and B are point sets lying in the two sides of the cut.$c_{p,q}$ is the capacity.

Find a cut to minimize 
![](images\mincut.png)

Weight of cut proportional to number of edges in the cut; tends to produce small, isolated components.
![](images\mincut1.png)

## Normalized Min Cut
Fix bias of Min Cut by normalizing for size of segments

$Ncut(A,B) = \frac{cut(A,B)}{volume(A)} + \frac{cut(A,B)}{volume(B)}$

volume(A) means sum of all edge weights that touch A
![](images\nmincut.png)


## Graph Cut in practice
* hard constraint: The user scribbles on the foreground and background. 
![](images\hard.png)

likelihood of each pixel’s colour being part of the foreground or background
Using all scribbled pixels to build a colour model, then calculate 
Mahalanobis distance 
### cost
$E(f) = \displaystyle\sum_{p\in P}D_p(f_p)+\displaystyle\sum_{p,q\in N}V_{p,q}(f_p,f_q)$

where $f$ denotes the labelling choice of the pixel, $p,q$ is the pixels 
$V_{p,q}$ is the pairwise term, i.e. the cost of the cut
$D_p$ is the unary term. The sum of the distance of pixels labelled FG actually belonging to that FG colour model.  (And similarly for BG pixels and the BG model)

The graph cut concept generalises to multi-label segmentation
